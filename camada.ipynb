{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "7e43de71-1709-4ad9-8663-c0bfb07778b6",
      "cell_type": "code",
      "source": "\n\"\"\"\nMódulo de funções de ativação para redes neurais.\n\nCada função de ativação tem:\n- forward: calcula a ativação\n- derivada: usada no backpropagation (veremos depois)\n\"\"\"\n\nclass Ativacao:\n    \"\"\"\"Classe base para funções de ativação. \"\"\"\n\n    def forward(self, x):\n        \"\"\"Aplica a função de ativação \"\"\"\n        raise NotImplementedError\n\n    def derivada(self, x):\n        \"\"\"Calcula a derivada (para backpropagation) \"\"\"\n        raise NotImplementedError\n",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "id": "2201913f-e7e3-441a-b9ee-aab5be5877b8",
      "cell_type": "code",
      "source": "# === Funções de ativação\n\nclass Degrau(Ativacao):\n    \"\"\"\n    Função Degrau (Step Function).\n    Retorna 1 se x >= limiar, senão 0.\n    \"\"\"\n\n    def __init__(self, limiar=1.0):\n        self.limiar = limiar\n\n    def forward(self, x):\n        return 1 if x >= self.limiar else 0\n\n    def derivada(self, x):\n        # Degrau não é diferenciável, mas podemos retornar 0\n        return 0\n\nclass Sigmoid(Ativacao):\n    \"\"\"\n    Função Sigmoid (Logística).\n    Mapeia valores para range (0, 1).\n    \"\"\"\n\n    def forward(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def derivada(self, x):\n        s = self.forward(x)\n        return s * (1 - s)\n\nclass ReLU(Ativacao):\n    \"\"\"\n    Rectified Linear Unit.\n    Retorna max(0, x).\n    \"\"\"\n\n    def forward(self, x):\n        return max(0, x)\n\n    def derivada(self, x):\n        return 1 if 0 >= 0 else 0\n\nclass Tanh(Ativacao):\n    \"\"\"\n    Tangente Hiperbólica.\n    Mapeia valores para range (-1, 1).\n    \"\"\"\n\n    def forward(self, x):\n        return np.tanh(x)\n\n    def derivada(self, x):\n        return 1 - np.tanh(x)**2\n\nclass Linear(Ativacao):\n\n    def forward(self, x):\n        return x\n\n    def derivada(self, x):\n        s = self.foward(x)\n        return s\n    \n    ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "id": "7d08b9b9-49a5-42b8-9e4c-8fdf0bc3e7ce",
      "cell_type": "code",
      "source": "\"\"\"\nImplementação de um neurônio artificial\n\"\"\"\n\n# from activations import Ativacao, Degrau\n\nclass Neuronio:\n    \"\"\"\n    Um neurônio artificial com aprendizado supervisionado.\n    \n    Attributes:\n        pesos (np.ndarray): Pesos sinápticos\n        bias (float): Termo de viés\n        taxa_aprendizado (float): Taxa de aprendizado\n        ativacao (Ativacao): Função de ativação\n    \"\"\"\n    \n    def __init__(self, num_entradas, taxa_aprendizado=0.1, ativacao=None):\n    \n        \"\"\"\n        Inicializa o neurônio.\n        \n        Args:\n            num_entradas (int): Número de entradas\n            taxa_aprendizado (float): Taxa de aprendizado\n            ativacao (Ativacao): Função de ativação (padrão: Degrau)\n        \"\"\"\n        self.pesos = np.random.rand(num_entradas)\n        self.bias = 0.0\n        self.taxa_aprendizado = taxa_aprendizado\n        \n        # Se não passar ativação, usa Degrau por padrão\n        self.ativacao = ativacao if ativacao is not None else Degrau()\n\n\n    def forward(self, entrada):\n        \"\"\"\n        Propagação para frente.\n        \n        Args:\n            entrada: Vetor de entrada\n            \n        Returns:\n            tuple: (saida_bruta, saida_ativada)\n        \"\"\"\n\n        # Combinação linear\n        saida_bruta = np.dot(entrada, self.pesos) + self.bias\n\n        # Aplica ativação (agora vem do objeto de ativação)\n        saida_ativada = self.ativacao.forward(saida_bruta)\n\n        return saida_bruta, saida_ativada\n\n    def treinar(self, entrada, esperado):\n        \"\"\"\n        Treina o neurônio com um exemplo.\n        \n        Args:\n            entrada: Vetor de entrada\n            esperado: Saída esperada\n            \n        Returns:\n            tuple: (saida, erro)\n        \"\"\"\n        saida_bruta, saida = self.forward(entrada)\n        erro = esperado - saida\n\n        # Atualiza pesos\n        self.pesos += self.taxa_aprendizado * erro * np.array(entrada)\n        self.bias += self.taxa_aprendizado * erro\n\n        return saida, erro\n\n    def __repr__(self):\n        \"\"\"Representação em string do neurônio.\"\"\"\n        return f\"Neuronio(entradas={len(self.pesos)}, ativacao={self.ativacao.__class__.__name__})\"\n\n        \n    ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "id": "676eeeb5-bae6-4938-9204-e57e6191a8e7",
      "cell_type": "code",
      "source": "\"\"\"\nImplementação de uma camada de neurônios.\n\"\"\"\n\nimport numpy as np\n\nclass Camada:\n    \"\"\"\n    Uma camada (layer) de neurônios.\n    \n    Todos os neurônios da camada:\n    - Recebem a mesma entrada\n    - Podem ter ativações diferentes (ou mesma)\n    - Produzem um vetor de saídas\n    \n    Attributes:\n        neuronios (list): Lista de objetos Neuronio\n        num_neuronios (int): Quantidade de neurônios na camada\n        num_entradas (int): Número de entradas que cada neurônio recebe\n    \"\"\"\n    \n    def __init__(self, num_entradas, num_neuronios, taxa_aprendizado=0.1, ativacao=None):\n        \"\"\"\n        Inicializa a camada.\n        \n        Args:\n            num_entradas (int): Número de entradas para cada neurônio\n            num_neuronios (int): Quantidade de neurônios na camada\n            taxa_aprendizado (float): Taxa de aprendizado\n            ativacao (Ativacao): Função de ativação (padrão: Degrau)\n        \"\"\"\n        self.num_neuronios = num_neuronios\n        self.num_entradas = num_entradas\n        \n        # Cria uma lista de neurônios\n        self.neuronios = []\n        for _ in range(num_neuronios):\n            neuronio = Neuronio(\n                num_entradas=num_entradas,\n                taxa_aprendizado=taxa_aprendizado,\n                ativacao=ativacao if ativacao is not None else Degrau()\n            )\n            self.neuronios.append(neuronio)\n    \n    def forward(self, entrada):\n        \"\"\"\n        Propagação para frente através da camada.\n        \n        Args:\n            entrada: Vetor de entrada (mesmo para todos os neurônios)\n            \n        Returns:\n            tuple: (saidas_brutas, saidas_ativadas)\n                - saidas_brutas: lista com saídas antes da ativação\n                - saidas_ativadas: lista com saídas após ativação\n        \"\"\"\n        saidas_brutas = []\n        saidas_ativadas = []\n        \n        # Cada neurônio processa a mesma entrada\n        for neuronio in self.neuronios:\n            bruta, ativada = neuronio.forward(entrada)\n            saidas_brutas.append(bruta)\n            saidas_ativadas.append(ativada)\n        \n        return np.array(saidas_brutas), np.array(saidas_ativadas)\n    \n    def treinar(self, entrada, esperados):\n        \"\"\"\n        Treina todos os neurônios da camada.\n        \n        Args:\n            entrada: Vetor de entrada\n            esperados: Lista de saídas esperadas (uma para cada neurônio)\n            \n        Returns:\n            tuple: (saidas, erros)\n        \"\"\"\n        if len(esperados) != self.num_neuronios:\n            raise ValueError(\n                f\"esperados deve ter {self.num_neuronios} valores, \"\n                f\"mas recebeu {len(esperados)}\"\n            )\n        \n        saidas = []\n        erros = []\n        \n        # Treina cada neurônio com sua saída esperada\n        for neuronio, esperado in zip(self.neuronios, esperados):\n            saida, erro = neuronio.treinar(entrada, esperado)\n            saidas.append(saida)\n            erros.append(erro)\n        \n        return np.array(saidas), np.array(erros)\n    \n    def get_pesos(self):\n        \"\"\"\n        Retorna os pesos de todos os neurônios como matriz.\n        \n        Returns:\n            np.ndarray: Matriz de pesos (num_neuronios x num_entradas)\n        \"\"\"\n        pesos_matriz = []\n        for neuronio in self.neuronios:\n            pesos_matriz.append(neuronio.pesos)\n        return np.array(pesos_matriz)\n    \n    def get_bias(self):\n        \"\"\"\n        Retorna os bias de todos os neurônios.\n        \n        Returns:\n            np.ndarray: Vetor de bias\n        \"\"\"\n        return np.array([neuronio.bias for neuronio in self.neuronios])\n    \n    def __repr__(self):\n        \"\"\"Representação em string da camada.\"\"\"\n        ativacao_nome = self.neuronios[0].ativacao.__class__.__name__\n        return (\n            f\"Camada(neuronios={self.num_neuronios}, \"\n            f\"entradas={self.num_entradas}, \"\n            f\"ativacao={ativacao_nome})\"\n        )\n    \n    def __len__(self):\n        \"\"\"Retorna o número de neurônios.\"\"\"\n        return self.num_neuronios\n\n\nclass CamadaVetorizada:\n    \"\"\"\n    Versão OTIMIZADA da camada usando operações vetorizadas.\n    \n    Muito mais rápida que criar neurônios individuais!\n    Essa é a abordagem usada em bibliotecas profissionais.\n    \"\"\"\n    \n    def __init__(self, num_entradas, num_neuronios, taxa_aprendizado=0.1, ativacao=None):\n        \"\"\"\n        Inicializa a camada vetorizada.\n        \n        Args:\n            num_entradas (int): Dimensão da entrada\n            num_neuronios (int): Número de neurônios (dimensão da saída)\n            taxa_aprendizado (float): Taxa de aprendizado\n            ativacao (Ativacao): Função de ativação\n        \"\"\"\n        self.num_neuronios = num_neuronios\n        self.num_entradas = num_entradas\n        self.taxa_aprendizado = taxa_aprendizado\n        self.ativacao = ativacao if ativacao is not None else Degrau()\n        \n        # Matriz de pesos: (num_neuronios x num_entradas)\n        # Cada LINHA é um neurônio\n        self.pesos = np.random.rand(num_neuronios, num_entradas)\n        \n        # Vetor de bias: (num_neuronios,)\n        self.bias = np.zeros(num_neuronios)\n    \n    def forward(self, entrada):\n        \"\"\"\n        Propagação vetorizada (MUITO mais rápida!).\n        \n        Args:\n            entrada: Vetor de entrada (num_entradas,)\n            \n        Returns:\n            tuple: (saidas_brutas, saidas_ativadas)\n        \"\"\"\n        # Multiplicação matricial: (num_neuronios x num_entradas) @ (num_entradas,)\n        # Resultado: (num_neuronios,)\n        saidas_brutas = self.pesos @ entrada + self.bias\n        \n        # Aplica ativação em cada elemento\n        saidas_ativadas = np.array([\n            self.ativacao.forward(x) for x in saidas_brutas\n        ])\n        \n        return saidas_brutas, saidas_ativadas\n    \n    def treinar(self, entrada, esperados):\n        \"\"\"\n        Treinamento vetorizado.\n        \n        Args:\n            entrada: Vetor de entrada\n            esperados: Vetor de saídas esperadas\n            \n        Returns:\n            tuple: (saidas, erros)\n        \"\"\"\n        saidas_brutas, saidas = self.forward(entrada)\n        erros = esperados - saidas\n        \n        # Atualização vetorizada dos pesos\n        # Cada linha (neurônio) é atualizada: peso += taxa * erro * entrada\n        entrada_array = np.array(entrada)\n        for i in range(self.num_neuronios):\n            self.pesos[i] += self.taxa_aprendizado * erros[i] * entrada_array\n            self.bias[i] += self.taxa_aprendizado * erros[i]\n        \n        return saidas, erros\n    \n    def __repr__(self):\n        return (\n            f\"CamadaVetorizada(neuronios={self.num_neuronios}, \"\n            f\"entradas={self.num_entradas}, \"\n            f\"ativacao={self.ativacao.__class__.__name__})\"\n        )",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "id": "c556ce43-6e35-4532-b700-1423c4118b48",
      "cell_type": "code",
      "source": "\"\"\"\nTestes para a classe Camada.\n\"\"\"\n\nimport numpy as np\n\n\nprint(\"=\" * 60)\nprint(\"TESTE 1: Camada com 3 neurônios\")\nprint(\"=\" * 60)\n\n# Cria uma camada: 10 entradas → 3 neurônios\ncamada = Camada(\n    num_entradas=10,\n    num_neuronios=3,\n    taxa_aprendizado=0.1,\n    ativacao=Sigmoid()\n)\n\nprint(f\"\\n{camada}\")\nprint(f\"Número de neurônios: {len(camada)}\")\n\n# Entrada de teste\nentrada = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n\n# Saídas esperadas para cada neurônio\nesperados = [1, 0, 1]  # Neurônio 1: quer 1, Neurônio 2: quer 0, Neurônio 3: quer 1\n\nprint(\"\\n--- Treinamento ---\")\nfor epoca in range(10):\n    saidas, erros = camada.treinar(entrada, esperados)\n    print(f\"Época {epoca} | saídas={saidas} | erros={erros}\")\n\n# Testa após treinamento\nprint(\"\\n--- Teste Final ---\")\n_, saidas_finais = camada.forward(entrada)\nprint(f\"Entrada: {entrada}\")\nprint(f\"Saídas: {saidas_finais}\")\nprint(f\"Esperado: {esperados}\")\n\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TESTE 2: Comparando Camada vs CamadaVetorizada\")\nprint(\"=\" * 60)\n\n# Mesma configuração para ambas\nentrada = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\nesperados = np.array([1, 0, 1])\n\n# Camada normal\ncamada_normal = Camada(10, 3, taxa_aprendizado=0.1, ativacao=Sigmoid())\n\n# Camada vetorizada\ncamada_vet = CamadaVetorizada(10, 3, taxa_aprendizado=0.1, ativacao=Sigmoid())\n\nprint(\"\\nTreinando ambas por 100 épocas...\")\n\n# Treina camada normal\nfor _ in range(100):\n    camada_normal.treinar(entrada, esperados)\n\n# Treina camada vetorizada\nfor _ in range(100):\n    camada_vet.treinar(entrada, esperados)\n\n# Compara resultados\n_, saidas_normal = camada_normal.forward(entrada)\n_, saidas_vet = camada_vet.forward(entrada)\n\nprint(f\"\\nCamada Normal: {saidas_normal}\")\nprint(f\"Camada Vetorizada: {saidas_vet}\")\nprint(f\"Esperado: {esperados}\")\n\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TESTE 3: Visualizando Pesos da Camada\")\nprint(\"=\" * 60)\n\ncamada = Camada(5, 2, ativacao=ReLU())\n\nprint(f\"\\nPesos iniciais:\")\nprint(camada.get_pesos())\nprint(f\"\\nBias iniciais:\")\nprint(camada.get_bias())\n\n# Treina um pouco\nentrada = [1, 0, 1, 0, 1]\nesperados = [1, 0]\n\nfor _ in range(50):\n    camada.treinar(entrada, esperados)\n\nprint(f\"\\nPesos após treinamento:\")\nprint(camada.get_pesos())\nprint(f\"\\nBias após treinamento:\")\nprint(camada.get_bias())",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "============================================================\nTESTE 1: Camada com 3 neurônios\n============================================================\n"
        },
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'Neuronio' is not defined",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Cria uma camada: 10 entradas → 3 neurônios\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m camada = \u001b[43mCamada\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_entradas\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_neuronios\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtaxa_aprendizado\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mativacao\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcamada\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNúmero de neurônios: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(camada)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mCamada.__init__\u001b[39m\u001b[34m(self, num_entradas, num_neuronios, taxa_aprendizado, ativacao)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mself\u001b[39m.neuronios = []\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_neuronios):\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     neuronio = \u001b[43mNeuronio\u001b[49m(\n\u001b[32m     39\u001b[39m         num_entradas=num_entradas,\n\u001b[32m     40\u001b[39m         taxa_aprendizado=taxa_aprendizado,\n\u001b[32m     41\u001b[39m         ativacao=ativacao \u001b[38;5;28;01mif\u001b[39;00m ativacao \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m Degrau()\n\u001b[32m     42\u001b[39m     )\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mself\u001b[39m.neuronios.append(neuronio)\n",
            "\u001b[31mNameError\u001b[39m: name 'Neuronio' is not defined"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 9
    }
  ]
}