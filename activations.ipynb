{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "ee03b3c8-68d5-43cd-b01a-91a8351bd0c0",
      "cell_type": "code",
      "source": "\n\"\"\"\nMódulo de funções de ativação para redes neurais.\n\nCada função de ativação tem:\n- forward: calcula a ativação\n- derivada: usada no backpropagation (veremos depois)\n\"\"\"\n\nimport numpy as np\n\nclass Ativacao:\n    \"\"\"\"Classe base para funções de ativação. \"\"\"\n\n    def forward(self, x):\n        \"\"\"Aplica a função de ativação \"\"\"\n        raise NotImplementedError\n\n    def derivada(self, x):\n        \"\"\"Calcula a derivada (para backpropagation) \"\"\"\n        raise NotImplementedError\n",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "id": "6526fe95-f454-4384-a181-1c906aca7f64",
      "cell_type": "code",
      "source": "# === Funções de ativação\n\nclass Degrau(Ativacao):\n    \"\"\"\n    Função Degrau (Step Function).\n    Retorna 1 se x >= limiar, senão 0.\n    \"\"\"\n\n    def __init__(self, limiar=1.0):\n        self.limiar = limiar\n\n    def forward(self, x):\n        return 1 if x >= self.limiar else 0\n\n    def derivada(self, x):\n        # Degrau não é diferenciável, mas podemos retornar 0\n        return 0\n\nclass Sigmoid(Ativacao):\n    \"\"\"\n    Função Sigmoid (Logística).\n    Mapeia valores para range (0, 1).\n    \"\"\"\n\n    def forward(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def derivada(self, x):\n        s = self.forward(x)\n        return s * (1 - s)\n\nclass ReLU(Ativacao):\n    \"\"\"\n    Rectified Linear Unit.\n    Retorna max(0, x).\n    \"\"\"\n\n    def forward(self, x):\n        return max(0, x)\n\n    def derivada(self, x):\n        return 1 if 0 >= 0 else 0\n\nclass Tanh(Ativacao):\n    \"\"\"\n    Tangente Hiperbólica.\n    Mapeia valores para range (-1, 1).\n    \"\"\"\n\n    def forward(self, x):\n        return np.tanh(x)\n\n    def derivada(self, x):\n        return 1 - np.tanh(x)**2\n\nclass Linear(Ativacao):\n\n    def forward(self, x):\n        return x\n\n    def derivada(self, x):\n        s = self.foward(x)\n        return s\n    \n    ",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": 36
    },
    {
      "id": "8a26cbb7-0be9-4f5c-9625-c598317a7de9",
      "cell_type": "code",
      "source": "# ==== Funções auxiliares para compatibilidade ====\n\ndef criar_ativacao(nome):\n    \"\"\"\n    Factory function para criar ativações por nome.\n    \n    Args:\n        nome (str): Nome da ativação ('degrau', 'sigmoid', 'relu', 'tanh')\n    \n    Returns:\n        Ativacao: Objeto da função de ativação\n    \"\"\"\n\n    ativacoes = {\n        'degrau': Degrau,\n        'sigmoid':Sigmoid,\n        'relu': ReLU,\n        'tanh': Tanh,\n        'linear': Linear\n    }\n\n    if nome.lower() not in ativacoes:\n        raise ValueError(f\"Ativação '{nome}' não encontrada. Opções {list(ativacoes.keys())}\")\n\n    return ativacoes[nome.lower()]()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 37
    },
    {
      "id": "e209881a-8776-4b93-9f34-ff9d8c6d202e",
      "cell_type": "code",
      "source": "\"\"\"\nImplementação de um neurônio artificial\n\"\"\"\n\n# from activations import Ativacao, Degrau\n\nclass Neuronio:\n    \"\"\"\n    Um neurônio artificial com aprendizado supervisionado.\n    \n    Attributes:\n        pesos (np.ndarray): Pesos sinápticos\n        bias (float): Termo de viés\n        taxa_aprendizado (float): Taxa de aprendizado\n        ativacao (Ativacao): Função de ativação\n    \"\"\"\n    \n    def __init__(self, num_entradas, taxa_aprendizado=0.1, ativacao=None):\n    \n        \"\"\"\n        Inicializa o neurônio.\n        \n        Args:\n            num_entradas (int): Número de entradas\n            taxa_aprendizado (float): Taxa de aprendizado\n            ativacao (Ativacao): Função de ativação (padrão: Degrau)\n        \"\"\"\n        self.pesos = np.random.rand(num_entradas)\n        self.bias = 0.0\n        self.taxa_aprendizado = taxa_aprendizado\n        \n        # Se não passar ativação, usa Degrau por padrão\n        self.ativacao = ativacao if ativacao is not None else Degrau()\n\n\n    def forward(self, entrada):\n        \"\"\"\n        Propagação para frente.\n        \n        Args:\n            entrada: Vetor de entrada\n            \n        Returns:\n            tuple: (saida_bruta, saida_ativada)\n        \"\"\"\n\n        # Combinação linear\n        saida_bruta = np.dot(entrada, self.pesos) + self.bias\n\n        # Aplica ativação (agora vem do objeto de ativação)\n        saida_ativada = self.ativacao.forward(saida_bruta)\n\n        return saida_bruta, saida_ativada\n\n    def treinar(self, entrada, esperado):\n        \"\"\"\n        Treina o neurônio com um exemplo.\n        \n        Args:\n            entrada: Vetor de entrada\n            esperado: Saída esperada\n            \n        Returns:\n            tuple: (saida, erro)\n        \"\"\"\n        saida_bruta, saida = self.forward(entrada)\n        erro = esperado - saida\n\n        # Atualiza pesos\n        self.pesos += self.taxa_aprendizado * erro * np.array(entrada)\n        self.bias += self.taxa_aprendizado * erro\n\n        return saida, erro\n\n    def __repr__(self):\n        \"\"\"Representação em string do neurônio.\"\"\"\n        return f\"Neuronio(entradas={len(self.pesos)}, ativacao={self.ativacao.__class__.__name__})\"\n\n        \n    ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "id": "ee9111ae-3de8-4224-90be-b50ba688e72e",
      "cell_type": "code",
      "source": "\"\"\"\nScript de teste para o neurônio com diferentes ativações.\n\"\"\"\n\n# === TEST 1: Neurônio com Degrau (padrão) ===\nprint(\"=\" * 50)\nprint(\"TEST 1: Neurônio com função Degrau\")\nprint(\"=\" * 50)\n\ndata = [0,1,0,0,0,0,0,0,0,0]\nesperado = 1\n\nneuronio_degrau = Neuronio(num_entradas=10, taxa_aprendizado=0.1)\nprint(neuronio_degrau)\n\nfor epoca in range(10):\n    saida, erro = neuronio_degrau.treinar(data, esperado)\n    print(f\"Época {epoca} | saída={saida} | erro={erro}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "==================================================\nTEST 1: Neurônio com função Degrau\n==================================================\nNeuronio(entradas=10, ativacao=Degrau)\nÉpoca 0 | saída=0 | erro=1\nÉpoca 1 | saída=0 | erro=1\nÉpoca 2 | saída=0 | erro=1\nÉpoca 3 | saída=0 | erro=1\nÉpoca 4 | saída=0 | erro=1\nÉpoca 5 | saída=1 | erro=0\nÉpoca 6 | saída=1 | erro=0\nÉpoca 7 | saída=1 | erro=0\nÉpoca 8 | saída=1 | erro=0\nÉpoca 9 | saída=1 | erro=0\n"
        }
      ],
      "execution_count": 40
    },
    {
      "id": "1bae88b1-c000-48e5-aea3-f52e2ff8e0eb",
      "cell_type": "code",
      "source": "# ===== TESTE 2: Neurônio com Sigmoid =====\nprint(\"\\n\" + \"=\" * 50)\nprint(\"TESTE 2: Neurônio com função Sigmoid\")\nprint(\"=\" * 50)\n\nneuronio_sigmoid = Neuronio(\n    num_entradas=10, \n    taxa_aprendizado=0.1,\n    ativacao=Sigmoid()  # ← Passamos a ativação!\n)\nprint(neuronio_sigmoid)\n\nfor epoca in range(10):\n    saida, erro = neuronio_sigmoid.treinar(data, esperado)\n    print(f\"Época {epoca} | saída={saida:.4f} | erro={erro:.4f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n==================================================\nTESTE 2: Neurônio com função Sigmoid\n==================================================\nNeuronio(entradas=10, ativacao=Sigmoid)\nÉpoca 0 | saída=0.5073 | erro=0.4927\nÉpoca 1 | saída=0.5319 | erro=0.4681\nÉpoca 2 | saída=0.5551 | erro=0.4449\nÉpoca 3 | saída=0.5770 | erro=0.4230\nÉpoca 4 | saída=0.5975 | erro=0.4025\nÉpoca 5 | saída=0.6167 | erro=0.3833\nÉpoca 6 | saída=0.6346 | erro=0.3654\nÉpoca 7 | saída=0.6514 | erro=0.3486\nÉpoca 8 | saída=0.6671 | erro=0.3329\nÉpoca 9 | saída=0.6817 | erro=0.3183\n"
        }
      ],
      "execution_count": 28
    },
    {
      "id": "14a2b736-134d-4033-8448-102bd6f0e798",
      "cell_type": "code",
      "source": "# ===== TESTE 3: Comparando múltiplas ativações =====\nprint(\"\\n\" + \"=\" * 50)\nprint(\"TESTE 3: Comparando diferentes ativações\")\nprint(\"=\" * 50)\n\nativacoes_teste = ['degrau', 'sigmoid', 'relu', 'tanh', 'linear']\n                  #'degrau', 'sigmoid', 'relU', 'tanh'\n\nfor nome_ativacao in ativacoes_teste:\n    print(f\"\\n--- Testando {nome_ativacao.upper()} ---\")\n    \n    ativacao = criar_ativacao(nome_ativacao)\n    neuronio = Neuronio(num_entradas=10, taxa_aprendizado=0.1, ativacao=ativacao)\n    \n    # Treina por 5 épocas\n    for epoca in range(5):\n        saida, erro = neuronio.treinar(data, esperado)\n    \n    # Mostra resultado final\n    _, saida_final = neuronio.forward(data)\n    print(f\"Saída final: {saida_final}\")",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n==================================================\nTESTE 3: Comparando diferentes ativações\n==================================================\n\n--- Testando DEGRAU ---\nSaída final: 1\n\n--- Testando SIGMOID ---\nSaída final: 0.6214499425263434\n\n--- Testando RELU ---\nSaída final: 0.9383714350865727\n\n--- Testando TANH ---\nSaída final: 0.640907040360849\n\n--- Testando LINEAR ---\nSaída final: 0.9536696923479657\n"
        }
      ],
      "execution_count": 49
    },
    {
      "id": "fbdb30c5-e697-4880-b683-3695c85b9930",
      "cell_type": "code",
      "source": "# PyTorch faz exatamente isso!\nimport torch.nn as nn\n\n# Ativações separadas\nativacao = nn.ReLU()  # ou nn.Sigmoid(), nn.Tanh()...\n\n# Passadas para camadas\ncamada = nn.Linear(10, 1)\nsaida = ativacao(camada(entrada))  # ← Mesma ideia!\n",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "856fc50c-afdb-4972-8ff9-a494bd38e279",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Saída final: 0.9986959177732646\nErro: 0.9983698972165809\nSaída final: 0.9989567342186116\nErro: 0.9986959177732646\nSaída final: 0.9991653873748894\nErro: 0.9989567342186116\nSaída final: 0.9993323098999115\nErro: 0.9991653873748894\nSaída final: 0.9994658479199292\nErro: 0.9993323098999115\n"
        }
      ],
      "execution_count": 54
    }
  ]
}