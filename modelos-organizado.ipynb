{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "1b9e9d06-2ce6-4f5f-ab03-fd38130e956b",
      "cell_type": "code",
      "source": "%reload_ext autoreload\n%autoreload 2",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "id": "7af8452b-5f8b-48f7-8b33-6b3c3d05905d",
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "id": "fa531405-1137-4bdd-a411-4542dbfa13d1",
      "cell_type": "code",
      "source": "# Grandes LLMs segue uma premicia de encapsulamento, cada neuronio e unico\n# E cada neuronio garda seu proprio valor, como pesos, bias ...\n# Não foca na linguagem python, mas no encapsulamento\n#Reutilização: Você pode criar múltiplos neurônios facilmente\n\n\n# === Métodos importantes\n# __init__ : Construtor - inicializa o neurônio\n# forward: Faz a propagação para frente (inferência)\n#  treinar: combina forward + backward (atualização)\n# _ativacao: método privado (prefixo_) para função de ativação",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "id": "6c4cb34e-5a24-4378-b5f0-9dea50747ae5",
      "cell_type": "code",
      "source": "import numpy as np\n\nclass Neuronio:\n    \"\"\"\n    Um neurônio artificial simples com aprendizado por descida de gradiente.\n    \n    Attributes:\n        pesos (np.ndarray): Pesos sinápticos do neurônio\n        bias (float): Termo de viés\n        taxa_aprendizado (float): Taxa de aprendizado para ajuste dos pesos\n    \"\"\"\n    \n    def __init__(self, num_entradas, taxa_aprendizado=0.1):\n        \"\"\"\n        Inicializa o neurônio com pesos aleatórios.\n        \n        Args:\n            num_entradas (int): Número de entradas que o neurônio receberá\n            taxa_aprendizado (float): Taxa para ajuste dos pesos\n        \"\"\"\n        self.pesos = np.random.rand(num_entradas)\n        self.bias = 0.0\n        self.taxa_aprendizado = taxa_aprendizado\n    \n    def forward(self, entrada):\n        \"\"\"\n        Calcula a saída do neurônio (propagação para frente).\n        \n        Args:\n            entrada (list ou np.ndarray): Vetor de entrada\n            \n        Returns:\n            tuple: (saida_bruta, saida_ativada)\n        \"\"\"\n        # Produto escalar: soma ponderada das entradas\n        soma = np.dot(entrada, self.pesos) + self.bias\n        \n        # Aplica função de ativação\n        saida_ativada = self._ativacao(soma)\n        \n        return soma, saida_ativada\n    \n    def _ativacao(self, x):\n        \"\"\"Função de ativação degrau (step function).\"\"\"\n        return 1 if x >= 1 else 0\n    \n    def treinar(self, entrada, esperado):\n        \"\"\"\n        Treina o neurônio com um exemplo.\n        \n        Args:\n            entrada: Vetor de entrada\n            esperado: Saída esperada (rótulo)\n            \n        Returns:\n            tuple: (saida, erro)\n        \"\"\"\n        # Forward pass\n        saida_bruta, saida = self.forward(entrada)\n        \n        # Calcula erro\n        erro = esperado - saida\n        \n        # Atualiza pesos (regra delta)\n        self.pesos += self.taxa_aprendizado * erro * np.array(entrada)\n        self.bias += self.taxa_aprendizado * erro\n        \n        return saida, erro\n\n\n    def ppt(self):\n        print(\"Bias: \",self.bias)\n        print(\"Pesos: \", self.pesos)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 111
    },
    {
      "id": "d9564c6c-6914-4493-86a2-6464d6b2a568",
      "cell_type": "code",
      "source": "\n# ===== USANDO A CLASSE =====\n\n# Dados de treinamento\ndata = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\nesperado = 1\n\n# Cria o neurônio\nneuronio = Neuronio(num_entradas=10, taxa_aprendizado=0.1)\n\nneuronio.ppt()",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Bias:  0.0\nPesos:  [0.0425155  0.96382705 0.46978669 0.92012753 0.52899929 0.69084313\n 0.46851712 0.74832461 0.71006207 0.98259734]\n"
        }
      ],
      "execution_count": 116
    },
    {
      "id": "fb008eb0-4a3b-4fc5-bb20-f89a0c58fd80",
      "cell_type": "code",
      "source": "\n\n# Treinamento\nprint(\"=== TREINAMENTO ===\")\nfor epoca in range(10):\n    saida, erro = neuronio.treinar(data, esperado)\n    print(f\"Época {epoca} | saída={saida} | erro={erro}\")\n\n# Testa o neurônio treinado\nprint(\"\\n=== TESTE ===\")\n_, saida_final = neuronio.forward(data)\nprint(f\"Saída final para {data}: {saida_final}\")\nprint(f\"Pesos finais: {neuronio.pesos}\")\nprint(f\"Bias final: {neuronio.bias}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "=== TREINAMENTO ===\nÉpoca 0 | saída=1 | erro=0\nÉpoca 1 | saída=1 | erro=0\nÉpoca 2 | saída=1 | erro=0\nÉpoca 3 | saída=1 | erro=0\nÉpoca 4 | saída=1 | erro=0\nÉpoca 5 | saída=1 | erro=0\nÉpoca 6 | saída=1 | erro=0\nÉpoca 7 | saída=1 | erro=0\nÉpoca 8 | saída=1 | erro=0\nÉpoca 9 | saída=1 | erro=0\n\n=== TESTE ===\nSaída final para [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]: 1\nPesos finais: [0.0425155  1.06382705 0.46978669 0.92012753 0.52899929 0.69084313\n 0.46851712 0.74832461 0.71006207 0.98259734]\nBias final: 0.1\n"
        }
      ],
      "execution_count": 153
    },
    {
      "id": "82fb1dce-fb97-4cdc-bda6-7f1a9b62b9c2",
      "cell_type": "code",
      "source": "# Teste com dados diferentes\n# A saída deve ser 0\n\nd = [0,1,0,0,0,0,0,0,0,0]\ne = 0\n\n\nprint(\"\\n=== TESTE ===\")\n_, saida_final = neuronio.forward(d)\nprint(f\"Saída final para {d}: {saida_final}\")\nprint(f\"Pesos finais: {neuronio.pesos}\")\nprint(f\"Bias final: {neuronio.bias}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n=== TESTE ===\nSaída final para [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]: 1\nPesos finais: [0.0425155  1.06382705 0.46978669 0.92012753 0.52899929 0.69084313\n 0.46851712 0.74832461 0.71006207 0.98259734]\nBias final: 0.1\n"
        }
      ],
      "execution_count": 146
    },
    {
      "id": "e1ef7e3b-dcd4-4322-a8bc-f5b9b873f3fd",
      "cell_type": "code",
      "source": "# Usando uma bibliteca profissional ficaria da seguinte forma:\n\nEquivalente em PyTorch (só pra você ver a similaridade)\nimport torch.nn as nn\n\nclass NeuronioTorch(nn.Module):\n    def __init__(self, num_entradas):\n        super().__init__()\n        self.linear = nn.Linear(num_entradas, 1)  # 1 neurônio\n    \n    def forward(self, x):\n        return torch.sigmoid(self.linear(x))  # Função de ativação\n        ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 150
    },
    {
      "id": "6a3f90e2-adca-451f-8605-eea749163638",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}